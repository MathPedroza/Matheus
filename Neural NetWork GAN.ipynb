{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyhPFIqtPoMMBwgop+ZpXb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MathPedroza/Matheus/blob/master/Neural%20NetWork%20GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**GAN**\n",
        "\n",
        "A estrutura geral de um GAN é mostrada no diagrama acima, usando imagens MNIST como dados. A amostra latente é um vetor aleatório que o gerador usa para construir suas imagens falsas.\n",
        "Isso geralmente é chamado de vetor latente e esse espaço vetorial é chamado de espaço latente . Enquanto o gerador treina, ele descobre como mapear vetores latentes para imagens reconhecíveis que podem enganar o discriminador."
      ],
      "metadata": {
        "id": "wT9e6-1dqYih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui será definido o número de subprocessos a serem usados ​​para o carregamento de dados\n",
        "Em seguida, defina quantas amostras por lote carregar, ou seja, tamanho do lote - o tamanho do lote ideal varia de 32 a 128.\n",
        "Em seguida, convertido os dados em torch.FloatTensor\n",
        "depois pegamos os conjuntos de dados de treinamento\n",
        "E prepare o carregador de dados que ajuda a carregar os dados no tamanho do lote mencionado acima."
      ],
      "metadata": {
        "id": "waRGpHLaqmt5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BTzayvCWnLHv"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# 1\n",
        "num_workers = 0\n",
        "# 2\n",
        "batch_size = 64\n",
        "# 3\n",
        "transform = transforms.ToTensor()\n",
        "# 4\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        " download=True, transform=transform)\n",
        "# 5\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        " num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui obtemos um lote de imagens de treinamento. Aqui, “dataiter” irá iterar por meio de imagens e rótulos que estão presentes no conjunto de dados. Mais tarde, você obtém uma imagem do lote."
      ],
      "metadata": {
        "id": "Iewp8TgLq3oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1 \n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "#2\n",
        "img = np.squeeze(images[0])\n",
        "fig = plt.figure(figsize = (3,3)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "Up9aBGjpnOOo",
        "outputId": "c3626555-fa6f-41a3-fd2d-5421778bc6f7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3c20772bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMUAAADDCAYAAAAyYdXtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAALpUlEQVR4nO3dbYxU9RXH8d8RywspihvTlSAUIQaDxG4TBWNJlVgqNBhcNcRNbEgg4As2wcaQEt6obTCkom2JpJGmKCQWMVHLSkzBAEIbGyIiPmGpxNi4BEEDyIMPBDh9MXftevY/7Ow8z/D9JGZmzt6993+Dv9x7/3P3XHN3Afi/i2o9AKDeEAogIBRAQCiAgFAAAaEAgpJCYWbTzGyfme03s8XlGhRQS1bs9xRmNkjSfyRNldQt6Q1JHe6+9zy/w5ciqBvubql6KUeKiZL2u/tH7n5a0nOSZpawPqAulBKKEZI+6fW5O6sBDe3iSm/AzOZLml/p7QDlUkooDkga2evzVVntO9x9laRVEtcUaAylnD69IekaM7vazAZLuldSV3mGBdRO0UcKdz9jZp2SNkkaJGm1u79ftpEBNVL0lGxRG+P0CXWkElOyQFMiFEBAKICAUAABoQACQgEEhAIICAUQEAogIBRAQCiAgFAAAaEAAkIBBIQCCAgFEBAKICAUQEAogIBQAEFJzdDM7GNJJySdlXTG3W8ox6AuNIMGDUrWL7vssrKsv7OzM1m/5JJL+tTGjRuXXHbBggXJ+vLly5P1jo6OZP3rr79O1pctW5asP/LII8l6JZWjQ+AUd/+8DOsB6gKnT0BQaihc0mYzezPrGQs0vFJPnya7+wEz+4GkV83s3+6+o/cCNFhGoynpSOHuB7LXw5JeUu6ZFXGZVe5+AxfhaBRFHynMbIiki9z9RPb+55J+U7aR1ZlRo0Yl64MHD07Wb7755j61yZMnJ5cdNmxYsn733XcXOLry6e7uTtZXrFiRrLe3tyfrJ06cSNbffvvtZH379u0FjK46Sjl9apX0kpn1rOev7v73sowKqKFSuo5/JOlHZRwLUBeYkgUCQgEEhAIIeGhL0NbWlqxv3bo1WS/X/Um1cO7cuT61OXPmJJc9efLkgNZ98ODBZP3o0aPJ+r59+wa0/nLgoS1AgQgFEBAKICAUQEAogIDZp6ClpSVZ37lzZ7I+ZsyYSg4nKd9Yjh07lqxPmTIlWT99+nSfWiPPpg0Us09AgQgFEBAKICAUQEAogKAcLW6aypEjR5L1RYsWJeszZsxI1t96660+tXx/vZbPnj17kvWpU6cm66dOnUrWr7vuumR94cKFAxrPhYIjBRAQCiAgFEBAKICg31CY2WozO2xm7/WqtZjZq2b2YfZ6eWWHCVRPv/c+mdlPJZ2UtNbdJ2S130k64u7LzGyxpMvd/df9bqwB7n0aqEsvvTRZT/U9euqpp5LLzp07N1m/7777kvV169YVODqcT9H3PmVtMOM85UxJa7L3ayTdWdLogDpS7DVFq7v3/BHup8o1RgOaQslf3rm7n++0iAbLaDTFHikOmdlwScpeD+dbkAbLaDTFHim6JM2WtCx73VC2ETWY48ePF7zsF198MaB1z5s3L1lfv359sp5qWYOBK2RKdp2kf0kaZ2bdZjZXuTBMNbMPJf0s+ww0hX6PFO6efqKfdFuZxwLUBb7RBgJCAQSEAghocVNFQ4YMSdZffvnlZP2WW25J1qdPn56sb968ubiBXaBocQMUiFAAAaEAAkIBBIQCCJh9qgNjx45N1nfv3p2s52ukvG3btmR9165dyfrKlSv71Kr5/0OtMfsEFIhQAAGhAAJCAQSEAgiYfapj7e3tyfrTTz+drA8dOnRA61+yZEmf2tq1a5PL5ntYfCNj9gkoEKEAAkIBBIQCCAgFEBTSYHm1pBmSDvdqsPywpHmSPssWW+Lur/S7MWafymLChAnJ+hNPPJGs33Zb4Y1X8jWBXrp0abJ+4MCBgtddb0qZfXpG0rRE/ffu3pb9128ggEZRbNdxoGmVck3RaWbvZA91yfvQFjObb2a7zCx9/zJQZ4oNxZ8kjZXUJumgpMfzLUiDZTSaokLh7ofc/ay7n5P0Z0kTyzssoHYKuvfJzEZL2thr9ml4z0NbzOxXkia5+70FrIfZpwoaNmxYsn7HHXck66l7qMySEzLaunVrsp7vQfeNIN/sU78NlrOu47dKusLMuiU9JOlWM2uT5JI+lnR/2UYK1FixXcf/UoGxAHWBb7SBgFAAAaEAAv7y7gL2zTff9KldfHH6MvPMmTPJ+u23356sv/baa0WPq1r4yzugQIQCCAgFEBAKICj24fKooeuvvz5Zv+eee5L1G2+8MVnPd1Gdsnfv3mR9x44dBa+jUXCkAAJCAQSEAggIBRAQCiBg9qkOjBs3Llnv7OxM1u+6665k/corryx5LGfPnk3W8zVYPnfuXMnbrDccKYCAUAABoQACQgEEhAIICunmMVLSWkmtynXvWOXufzSzFknrJY1WrqPHLHc/WrmhNpbUTFBHR6oHRP5ZptGjR5dzSH2kHjqfr5FyV1dXRcdSTwo5UpyR9KC7j5d0k6QFZjZe0mJJW9z9Gklbss9AwyukwfJBd9+dvT8h6QNJIyTNlLQmW2yNpDsrNUigmgb05V3WKfDHknZKau3pEijpU+VOr1K/M1/S/OKHCFRXwRfaZvZ9SS9IesDdj/f+mee6HySbEtBgGY2moFCY2feUC8Sz7v5iVj5kZsOznw+XdLgyQwSqq5DZJ1OuTeYH7t77+VFdkmZLWpa9bqjICOtEa2vy7FDjx49P1p988sk+tWuvvbasY4p27tyZrD/22GPJ+oYNff/JmvFepoEq5JriJ5J+KeldM9uT1ZYoF4bnzWyupP9KmlWZIQLVVUiD5X9KSvdnlwp/wiDQIPhGGwgIBRAQCiC4YP/yrqWlJVnP93D1tra2ZH3MmDFlG1P0+uuvJ+uPP55+7uamTZuS9a+++qpsY7oQcKQAAkIBBIQCCAgFEBAKIGia2adJkyYl64sWLUrWJ06cmKyPGDGibGOKvvzyy2R9xYoVyfqjjz6arJ86dapsY0JfHCmAgFAAAaEAAkIBBIQCCJpm9qm9vX1A9YHK98y3jRs3Juuph7Hnu2fp2LFjxQ8MZceRAggIBRAQCiAgFEBguT5m51kgf4PlhyXNk/RZtugSd3+ln3Wdf2NAFbl7siFHIaEYLmm4u+82s6GS3lSub+wsSSfdfXmhgyAUqCf5QlFIi5uDkg5m70+YWU+DZaApDeiaIjRYlqROM3vHzFab2eV5fme+me0ys74PQwDqUL+nT98umGuwvF3SUnd/0cxaJX2u3HXGb5U7xZrTzzo4fULdKPqaQvq2wfJGSZtCP9men4+WtNHdJ/SzHkKBupEvFP2ePuVrsNzTcTzTLum9UgcJ1INCZp8mS/qHpHcl9bSkXiKpQ1KbcqdPH0u6v9dDXPKtiyMF6kZJp0/lQihQT4o+fQIuNIQCCAgFEBAKICAUQEAogIBQAAGhAAJCAQTVbnHzuXLP3JakK7LPzY79rE8/zPeDqt7m8Z0Nm+1y9xtqsvEqYj8bD6dPQEAogKCWoVhVw21XE/vZYGp2TQHUK06fgKDqoTCzaWa2z8z2m9niam+/krKuJofN7L1etRYze9XMPsxek11PGomZjTSzbWa218zeN7OFWb0p9rWqoTCzQZJWSpouabykDjMbX80xVNgzkqaF2mJJW9z9Gklbss+N7oykB919vKSbJC3I/h2bYl+rfaSYKGm/u3/k7qclPSdpZpXHUDHuvkPSkVCeKWlN9n6Nct0VG5q7H3T33dn7E5J6GuQ1xb5WOxQjJH3S63O3mr/bYGuvhg6fKteTt2mEBnlNsa9caFeR56b6mma6L2uQ94KkB9z9eO+fNfK+VjsUBySN7PX5qqzWzA719MjKXg/XeDxlkTXIe0HSs+7+YlZuin2tdijekHSNmV1tZoMl3Supq8pjqLYuSbOz97MlbajhWMoiX4M8Ncm+Vv3LOzP7haQ/SBokabW7L63qACrIzNZJulW5O0YPSXpI0t8kPS9plHJ3CM9y93gx3lDO0yBvp5pgX/lGGwi40AYCQgEEhAIICAUQEAogIBRAQCiAgFAAwf8Ac0KUEmzQH7gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discriminador:**\n",
        "\n",
        "A rede discriminadora será um classificador linear bastante típico. Para tornar essa rede um aproximador de função universal, precisaremos de pelo menos uma camada oculta, e essas camadas ocultas devem ter um atributo-chave:\n",
        "\n",
        "Todas as camadas ocultas terão uma função de ativação Leaky ReLu aplicada às suas saídas.\n",
        "\n",
        "Devemos usar um ReLU com vazamento para permitir que os gradientes fluam para trás através da camada sem impedimentos. Um ReLU com vazamento é como um ReLU normal, exceto que há uma pequena saída diferente de zero para valores de entrada negativos.\n",
        "\n",
        "Aqui definimos todas as camadas lineares ocultas\n",
        "Em seguida, criamos uma camada final totalmente conectada\n",
        "Na função de avanço, você primeiro precisa achatar a imagem.\n",
        "E, em seguida, definimos todas as camadas ocultas\n",
        "Eventualmente, criando uma camada final"
      ],
      "metadata": {
        "id": "uEJgYONorAwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, input_size, hidden_dim, output_size):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    # 1\n",
        "    self.fc1 = nn.Linear(input_size, hidden_dim*4)\n",
        "    self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n",
        "    self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
        "\n",
        "    # 2\n",
        "    self.fc4 = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    # dropout layer\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 3\n",
        "      x = x.view(-1, 28*28)\n",
        "    # 4\n",
        "    x = F.leaky_relu(self.fc1(x), 0.2)  # (input, negative_slope=0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "    # 5\n",
        "    out = self.fc4(x)\n",
        "\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "tgI7lJ3ingCO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O gerador demonstrou ter o melhor desempenho com 𝑡𝑎𝑛ℎ para a saída do gerador, que dimensiona a saída para estar entre -1 e 1, em vez de 0 e 1.\n",
        "\n",
        "Aqui definimos todas as camadas lineares ocultas\n",
        "Em seguida, foi criado uma camada final totalmente conectada\n",
        "Adicione uma camada de eliminação para evitar overfitting\n",
        "Criamos todas as camadas ocultas na função de avanço\n",
        "Eventualmente, adicione uma camada final com tanh aplicado"
      ],
      "metadata": {
        "id": "tjZHN66HrQkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, input_size, hidden_dim, output_size):\n",
        "    super(Generator, self).__init__()\n",
        " \n",
        " # 1\n",
        "    self.fc1 = nn.Linear(input_size, hidden_dim)\n",
        "    self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)\n",
        "    self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)\n",
        " \n",
        " # 2\n",
        "    self.fc4 = nn.Linear(hidden_dim*4, output_size)\n",
        " \n",
        " # 3\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "def forward(self, x):\n",
        " # 4\n",
        "    x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        " # 5\n",
        "    out = F.tanh(self.fc4(x))\n",
        "    return out"
      ],
      "metadata": {
        "id": "B2OYxXcnopxp"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tamanho da imagem de entrada para discriminador (28 * 28 = 784)\n",
        "\n",
        "Tamanho da saída do discriminador (real ou falso)\n",
        "\n",
        "Tamanho da última camada oculta no discriminador\n",
        "\n",
        "Tamanho do vetor latente para dar ao gerador\n",
        "\n",
        "Tamanho da saída do discriminador (imagem gerada)\n",
        "Tamanho da primeira camada oculta no gerador"
      ],
      "metadata": {
        "id": "EKaIqwmVrjAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper parâmetros do Discriminador\n",
        "# 1\n",
        "input_size = 784\n",
        "# 2\n",
        "d_output_size = 1\n",
        "# 3\n",
        "d_hidden_size = 32\n",
        "# Hyper parâmetros do gerador\n",
        "# 4\n",
        "z_size = 100\n",
        "# 5\n",
        "g_output_size = 784\n",
        "# 6\n",
        "g_hidden_size = 32"
      ],
      "metadata": {
        "id": "z3wtD2ikpJV6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instaciando o Discriminador e gerador\n",
        "D = Discriminator(input_size, d_hidden_size, d_output_size)\n",
        "G = Generator(z_size, g_hidden_size, g_output_size)\n",
        "\n",
        "print(D)\n",
        "print()\n",
        "print(G)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "yUTV2HucpLdn",
        "outputId": "49334a72-cf5e-457a-8c9c-9caf64f5ef63"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-942b87943cae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Instaciando o Discriminador e gerador\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_output_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_output_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-7bc0979ac1ec>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_dim, output_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (input, negative_slope=0.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'x' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defindo as perdas:**\n",
        "\n",
        "Para o discriminador, a perda total é a soma das perdas para imagens reais e falsas, d_loss = d_real_loss + d_fake_loss.\n",
        "Lembre-se de que queremos que o discriminador produza 1 para imagens reais e 0 para imagens falsas, portanto, precisamos configurar as perdas para refletir isso.\n",
        "A perda do gerador será semelhante apenas com rótulos invertidos. O objetivo do gerador é obter D (fake_images) = 1.\n",
        "Nesse caso, os rótulos são invertidos para representar que o gerador está tentando enganar o discriminador fazendo-o pensar que as imagens que ele gera (falsificações) são reais!"
      ],
      "metadata": {
        "id": "fDrCVcpsrqJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando as perdas\n",
        "def real_loss(D_out, smooth=False):\n",
        "    batch_size = D_out.size(0)\n",
        "    # label smoothing\n",
        "    if smooth:\n",
        "        # smooth, real labels = 0.9\n",
        "    labels = torch.ones(batch_size)*0.9\n",
        "    else:\n",
        "    labels = torch.ones(batch_size)  # real labels = 1\n",
        "\n",
        "    # \n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # calculando as perdass\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def fake_loss(D_out):\n",
        "    batch_size = D_out.size(0)\n",
        "    labels = torch.zeros(batch_size)  # fake labels = 0\n",
        "    criCalculando as perdas\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "X9nwIbXArxLN",
        "outputId": "061e042d-a225-4ab8-f221-9cdfba66b6cb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-46-4e215f544927>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    labels = torch.ones(batch_size)*0.9\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "lr = 0.002\n",
        "d_optimizer = optim.Adam(D.parameters(), lr)\n",
        "g_optimizer = optim.Adam(G.parameters(), lr)"
      ],
      "metadata": {
        "id": "9Afi98nMr5EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O treinamento envolverá a alternância entre o treinamento do discriminador e do gerador. Usaremos nossas funções real_loss e fake_loss para nos ajudar a calcular as perdas do discriminador em todos os casos a seguir.\n",
        "\n",
        "**Treinamento de discriminador:**\n",
        "\n",
        "Calculando a perda do discriminador em imagens reais de treinamento\n",
        "Gerar imagens falsas\n",
        "Calculando a perda de discriminador em imagens falsas geradas\n",
        "Some a perda real e falsa\n",
        "Executando retropropagação + uma etapa de otimização para atualizar os pesos do discriminador\n",
        "Gerarando imagens falsas\n",
        "Calculando a perda de discriminador em imagens falsas, usando etiquetas invertidas!\n",
        "Execute retropropagação + uma etapa de otimização para atualizar os pesos do gerador"
      ],
      "metadata": {
        "id": "nNiXfxb_r6oX"
      }
    }
  ]
}