{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyhPFIqtPoMMBwgop+ZpXb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MathPedroza/Matheus/blob/master/Neural%20NetWork%20GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**GAN**\n",
        "\n",
        "A estrutura geral de um GAN √© mostrada no diagrama acima, usando imagens MNIST como dados. A amostra latente √© um vetor aleat√≥rio que o gerador usa para construir suas imagens falsas.\n",
        "Isso geralmente √© chamado de vetor latente e esse espa√ßo vetorial √© chamado de espa√ßo latente . Enquanto o gerador treina, ele descobre como mapear vetores latentes para imagens reconhec√≠veis que podem enganar o discriminador."
      ],
      "metadata": {
        "id": "wT9e6-1dqYih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui ser√° definido o n√∫mero de subprocessos a serem usados ‚Äã‚Äãpara o carregamento de dados\n",
        "Em seguida, defina quantas amostras por lote carregar, ou seja, tamanho do lote - o tamanho do lote ideal varia de 32 a 128.\n",
        "Em seguida, convertido os dados em torch.FloatTensor\n",
        "depois pegamos os conjuntos de dados de treinamento\n",
        "E prepare o carregador de dados que ajuda a carregar os dados no tamanho do lote mencionado acima."
      ],
      "metadata": {
        "id": "waRGpHLaqmt5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BTzayvCWnLHv"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# 1\n",
        "num_workers = 0\n",
        "# 2\n",
        "batch_size = 64\n",
        "# 3\n",
        "transform = transforms.ToTensor()\n",
        "# 4\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        " download=True, transform=transform)\n",
        "# 5\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        " num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui obtemos um lote de imagens de treinamento. Aqui, ‚Äúdataiter‚Äù ir√° iterar por meio de imagens e r√≥tulos que est√£o presentes no conjunto de dados. Mais tarde, voc√™ obt√©m uma imagem do lote."
      ],
      "metadata": {
        "id": "Iewp8TgLq3oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1 \n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "#2\n",
        "img = np.squeeze(images[0])\n",
        "fig = plt.figure(figsize = (3,3)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "Up9aBGjpnOOo",
        "outputId": "c3626555-fa6f-41a3-fd2d-5421778bc6f7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3c20772bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discriminador:**\n",
        "\n",
        "A rede discriminadora ser√° um classificador linear bastante t√≠pico. Para tornar essa rede um aproximador de fun√ß√£o universal, precisaremos de pelo menos uma camada oculta, e essas camadas ocultas devem ter um atributo-chave:\n",
        "\n",
        "Todas as camadas ocultas ter√£o uma fun√ß√£o de ativa√ß√£o Leaky ReLu aplicada √†s suas sa√≠das.\n",
        "\n",
        "Devemos usar um ReLU com vazamento para permitir que os gradientes fluam para tr√°s atrav√©s da camada sem impedimentos. Um ReLU com vazamento √© como um ReLU normal, exceto que h√° uma pequena sa√≠da diferente de zero para valores de entrada negativos.\n",
        "\n",
        "Aqui definimos todas as camadas lineares ocultas\n",
        "Em seguida, criamos uma camada final totalmente conectada\n",
        "Na fun√ß√£o de avan√ßo, voc√™ primeiro precisa achatar a imagem.\n",
        "E, em seguida, definimos todas as camadas ocultas\n",
        "Eventualmente, criando uma camada final"
      ],
      "metadata": {
        "id": "uEJgYONorAwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, input_size, hidden_dim, output_size):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    # 1\n",
        "    self.fc1 = nn.Linear(input_size, hidden_dim*4)\n",
        "    self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n",
        "    self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
        "\n",
        "    # 2\n",
        "    self.fc4 = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    # dropout layer\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 3\n",
        "      x = x.view(-1, 28*28)\n",
        "    # 4\n",
        "    x = F.leaky_relu(self.fc1(x), 0.2)  # (input, negative_slope=0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "    # 5\n",
        "    out = self.fc4(x)\n",
        "\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "tgI7lJ3ingCO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O gerador demonstrou ter o melhor desempenho com ùë°ùëéùëõ‚Ñé para a sa√≠da do gerador, que dimensiona a sa√≠da para estar entre -1 e 1, em vez de 0 e 1.\n",
        "\n",
        "Aqui definimos todas as camadas lineares ocultas\n",
        "Em seguida, foi criado uma camada final totalmente conectada\n",
        "Adicione uma camada de elimina√ß√£o para evitar overfitting\n",
        "Criamos todas as camadas ocultas na fun√ß√£o de avan√ßo\n",
        "Eventualmente, adicione uma camada final com tanh aplicado"
      ],
      "metadata": {
        "id": "tjZHN66HrQkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, input_size, hidden_dim, output_size):\n",
        "    super(Generator, self).__init__()\n",
        " \n",
        " # 1\n",
        "    self.fc1 = nn.Linear(input_size, hidden_dim)\n",
        "    self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)\n",
        "    self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)\n",
        " \n",
        " # 2\n",
        "    self.fc4 = nn.Linear(hidden_dim*4, output_size)\n",
        " \n",
        " # 3\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "def forward(self, x):\n",
        " # 4\n",
        "    x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        "    x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "    x = self.dropout(x)\n",
        " # 5\n",
        "    out = F.tanh(self.fc4(x))\n",
        "    return out"
      ],
      "metadata": {
        "id": "B2OYxXcnopxp"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tamanho da imagem de entrada para discriminador (28 * 28 = 784)\n",
        "\n",
        "Tamanho da sa√≠da do discriminador (real ou falso)\n",
        "\n",
        "Tamanho da √∫ltima camada oculta no discriminador\n",
        "\n",
        "Tamanho do vetor latente para dar ao gerador\n",
        "\n",
        "Tamanho da sa√≠da do discriminador (imagem gerada)\n",
        "Tamanho da primeira camada oculta no gerador"
      ],
      "metadata": {
        "id": "EKaIqwmVrjAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper par√¢metros do Discriminador\n",
        "# 1\n",
        "input_size = 784\n",
        "# 2\n",
        "d_output_size = 1\n",
        "# 3\n",
        "d_hidden_size = 32\n",
        "# Hyper par√¢metros do gerador\n",
        "# 4\n",
        "z_size = 100\n",
        "# 5\n",
        "g_output_size = 784\n",
        "# 6\n",
        "g_hidden_size = 32"
      ],
      "metadata": {
        "id": "z3wtD2ikpJV6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instaciando o Discriminador e gerador\n",
        "D = Discriminator(input_size, d_hidden_size, d_output_size)\n",
        "G = Generator(z_size, g_hidden_size, g_output_size)\n",
        "\n",
        "print(D)\n",
        "print()\n",
        "print(G)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "yUTV2HucpLdn",
        "outputId": "49334a72-cf5e-457a-8c9c-9caf64f5ef63"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-942b87943cae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Instaciando o Discriminador e gerador\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_output_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_output_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-7bc0979ac1ec>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_dim, output_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (input, negative_slope=0.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'x' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defindo as perdas:**\n",
        "\n",
        "Para o discriminador, a perda total √© a soma das perdas para imagens reais e falsas, d_loss = d_real_loss + d_fake_loss.\n",
        "Lembre-se de que queremos que o discriminador produza 1 para imagens reais e 0 para imagens falsas, portanto, precisamos configurar as perdas para refletir isso.\n",
        "A perda do gerador ser√° semelhante apenas com r√≥tulos invertidos. O objetivo do gerador √© obter D (fake_images) = 1.\n",
        "Nesse caso, os r√≥tulos s√£o invertidos para representar que o gerador est√° tentando enganar o discriminador fazendo-o pensar que as imagens que ele gera (falsifica√ß√µes) s√£o reais!"
      ],
      "metadata": {
        "id": "fDrCVcpsrqJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando as perdas\n",
        "def real_loss(D_out, smooth=False):\n",
        "    batch_size = D_out.size(0)\n",
        "    # label smoothing\n",
        "    if smooth:\n",
        "        # smooth, real labels = 0.9\n",
        "    labels = torch.ones(batch_size)*0.9\n",
        "    else:\n",
        "    labels = torch.ones(batch_size)  # real labels = 1\n",
        "\n",
        "    # \n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # calculando as perdass\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def fake_loss(D_out):\n",
        "    batch_size = D_out.size(0)\n",
        "    labels = torch.zeros(batch_size)  # fake labels = 0\n",
        "    criCalculando as perdas\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "X9nwIbXArxLN",
        "outputId": "061e042d-a225-4ab8-f221-9cdfba66b6cb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-46-4e215f544927>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    labels = torch.ones(batch_size)*0.9\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "lr = 0.002\n",
        "d_optimizer = optim.Adam(D.parameters(), lr)\n",
        "g_optimizer = optim.Adam(G.parameters(), lr)"
      ],
      "metadata": {
        "id": "9Afi98nMr5EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O treinamento envolver√° a altern√¢ncia entre o treinamento do discriminador e do gerador. Usaremos nossas fun√ß√µes real_loss e fake_loss para nos ajudar a calcular as perdas do discriminador em todos os casos a seguir.\n",
        "\n",
        "**Treinamento de discriminador:**\n",
        "\n",
        "Calculando a perda do discriminador em imagens reais de treinamento\n",
        "Gerar imagens falsas\n",
        "Calculando a perda de discriminador em imagens falsas geradas\n",
        "Some a perda real e falsa\n",
        "Executando retropropaga√ß√£o + uma etapa de otimiza√ß√£o para atualizar os pesos do discriminador\n",
        "Gerarando imagens falsas\n",
        "Calculando a perda de discriminador em imagens falsas, usando etiquetas invertidas!\n",
        "Execute retropropaga√ß√£o + uma etapa de otimiza√ß√£o para atualizar os pesos do gerador"
      ],
      "metadata": {
        "id": "nNiXfxb_r6oX"
      }
    }
  ]
}